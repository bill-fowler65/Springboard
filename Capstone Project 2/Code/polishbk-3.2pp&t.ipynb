{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9b9af03",
   "metadata": {},
   "source": [
    "# Capstone 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba1dd20",
   "metadata": {},
   "source": [
    "## Polish Bankruptcy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571dad5a",
   "metadata": {},
   "source": [
    "## Pre-processing and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfa3a73",
   "metadata": {},
   "source": [
    "In the pre-processing step I will impute the missing values and from the data, drop redundent high correlation variables from the dataset, complete a stratified train test split and and investigate scaling the training data.  Because of the high amount of skewed data the possibility of transforming the data may need to be done after the initial models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b0a39",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "573e8927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from random import randint, choice\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7fbdfd",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46121cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 111 entries, 0 to 110\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Feature1     111 non-null    object \n",
      " 1   Feature2     111 non-null    object \n",
      " 2   Correlation  111 non-null    float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 2.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#load bankruptcy data and column key data\n",
    "bankruptcy_data = pd.read_csv('bankruptcy_data_comb.csv')\n",
    "data_columns = pd.read_csv('column_key.csv')\n",
    "\n",
    "#load high correlation dataframe\n",
    "dataCorrhigh = pd.read_csv('dataCorrhigh.csv')\n",
    "\n",
    "#display high correlation pairs dataframe\n",
    "print(dataCorrhigh.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4917aaaa",
   "metadata": {},
   "source": [
    "## Impute missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178b09d",
   "metadata": {},
   "source": [
    "Based of the distribution of data and the number of outliers, I have made the decsion to imput the missing values based on the median of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aa4bad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43405 entries, 0 to 43404\n",
      "Data columns (total 66 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   X01     43405 non-null  float64\n",
      " 1   X02     43405 non-null  float64\n",
      " 2   X03     43405 non-null  float64\n",
      " 3   X04     43405 non-null  float64\n",
      " 4   X05     43405 non-null  float64\n",
      " 5   X06     43405 non-null  float64\n",
      " 6   X07     43405 non-null  float64\n",
      " 7   X08     43405 non-null  float64\n",
      " 8   X09     43405 non-null  float64\n",
      " 9   X10     43405 non-null  float64\n",
      " 10  X11     43405 non-null  float64\n",
      " 11  X12     43405 non-null  float64\n",
      " 12  X13     43405 non-null  float64\n",
      " 13  X14     43405 non-null  float64\n",
      " 14  X15     43405 non-null  float64\n",
      " 15  X16     43405 non-null  float64\n",
      " 16  X17     43405 non-null  float64\n",
      " 17  X18     43405 non-null  float64\n",
      " 18  X19     43405 non-null  float64\n",
      " 19  X20     43405 non-null  float64\n",
      " 20  X21     43405 non-null  float64\n",
      " 21  X22     43405 non-null  float64\n",
      " 22  X23     43405 non-null  float64\n",
      " 23  X24     43405 non-null  float64\n",
      " 24  X25     43405 non-null  float64\n",
      " 25  X26     43405 non-null  float64\n",
      " 26  X27     43405 non-null  float64\n",
      " 27  X28     43405 non-null  float64\n",
      " 28  X29     43405 non-null  float64\n",
      " 29  X30     43405 non-null  float64\n",
      " 30  X31     43405 non-null  float64\n",
      " 31  X32     43405 non-null  float64\n",
      " 32  X33     43405 non-null  float64\n",
      " 33  X34     43405 non-null  float64\n",
      " 34  X35     43405 non-null  float64\n",
      " 35  X36     43405 non-null  float64\n",
      " 36  X37     43405 non-null  float64\n",
      " 37  X38     43405 non-null  float64\n",
      " 38  X39     43405 non-null  float64\n",
      " 39  X40     43405 non-null  float64\n",
      " 40  X41     43405 non-null  float64\n",
      " 41  X42     43405 non-null  float64\n",
      " 42  X43     43405 non-null  float64\n",
      " 43  X44     43405 non-null  float64\n",
      " 44  X45     43405 non-null  float64\n",
      " 45  X46     43405 non-null  float64\n",
      " 46  X47     43405 non-null  float64\n",
      " 47  X48     43405 non-null  float64\n",
      " 48  X49     43405 non-null  float64\n",
      " 49  X50     43405 non-null  float64\n",
      " 50  X51     43405 non-null  float64\n",
      " 51  X52     43405 non-null  float64\n",
      " 52  X53     43405 non-null  float64\n",
      " 53  X54     43405 non-null  float64\n",
      " 54  X55     43405 non-null  float64\n",
      " 55  X56     43405 non-null  float64\n",
      " 56  X57     43405 non-null  float64\n",
      " 57  X58     43405 non-null  float64\n",
      " 58  X59     43405 non-null  float64\n",
      " 59  X60     43405 non-null  float64\n",
      " 60  X61     43405 non-null  float64\n",
      " 61  X62     43405 non-null  float64\n",
      " 62  X63     43405 non-null  float64\n",
      " 63  X64     43405 non-null  float64\n",
      " 64  Class   43405 non-null  int64  \n",
      " 65  Year    43405 non-null  int64  \n",
      "dtypes: float64(64), int64(2)\n",
      "memory usage: 21.9 MB\n"
     ]
    }
   ],
   "source": [
    "#create new dataframe by imputing missing values with the median\n",
    "bankruptcy_complete = bankruptcy_data.apply(lambda x: x.fillna(x.median()),axis=0)\n",
    "\n",
    "#display complete dataframe\n",
    "bankruptcy_complete.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb2ae63",
   "metadata": {},
   "source": [
    "## Drop redundant high correlation  columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "393c7d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X58', 'X46', 'X58', 'X56', 'X17', 'X23', 'X31', 'X43', 'X38', 'X53', 'X23', 'X51', 'X50', 'X50', 'X14', 'X36', 'X56', 'X43', 'X43', 'X16', 'X56', 'X58', 'X44', 'X33', 'X22', 'X16', 'X22', 'X26', 'X44', 'X43', 'X20', 'X56', 'X49', 'X48', 'X63', 'X49', 'X32', 'X34', 'X47', 'X63', 'X63', 'X02', 'X51', 'X62', 'X54', 'X64', 'X44', 'X30', 'X22', 'X11', 'X33', 'X22', 'X35', 'X62', 'X09', 'X11', 'X14', 'X56', 'X30', 'X20', 'X63', 'X35', 'X11', 'X22', 'X33', 'X35', 'X33', 'X18', 'X62', 'X43', 'X48', 'X48', 'X20', 'X56', 'X58', 'X48', 'X25', 'X38', 'X06', 'X34', 'X36', 'X35', 'X35', 'X11', 'X24', 'X06', 'X51', 'X24', 'X24', 'X24', 'X34', 'X24', 'X13', 'X13', 'X20', 'X11', 'X34', 'X35', 'X43', 'X63', 'X44', 'X13', 'X18']\n"
     ]
    }
   ],
   "source": [
    "# drop first row of correlation dataframe with correlation of 1.  Data is identical \n",
    "dataCorrhigh = dataCorrhigh.drop(dataCorrhigh.index[0]).reset_index(drop=True)\n",
    "\n",
    "#create empty final high correlation dataframe\n",
    "corr_high_final = []\n",
    "\n",
    "# remove redundant feature X07\n",
    "dataCorrhigh = dataCorrhigh.drop(dataCorrhigh[(dataCorrhigh['Feature1'] == 'X07') | (dataCorrhigh['Feature2'] == 'X07')].index,axis = 0 ,inplace = False)\n",
    "dataCorrhigh = dataCorrhigh.reset_index(drop=True)\n",
    "\n",
    "# iterate over high correlation dataframe and use random number to determine final feture dataframe\n",
    "for x in range(len(dataCorrhigh)):\n",
    "    k = random.randint(0, 1)\n",
    "    if k == 0:\n",
    "        corr_high_final.append(dataCorrhigh['Feature1'][x])\n",
    "        \n",
    "    else:\n",
    "        corr_high_final.append(dataCorrhigh['Feature2'][x])\n",
    "   \n",
    "print(corr_high_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2997699",
   "metadata": {},
   "source": [
    "## Create final feature array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a69759c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X02' 'X06' 'X09' 'X11' 'X13' 'X14' 'X16' 'X17' 'X18' 'X20' 'X22' 'X23'\n",
      " 'X24' 'X25' 'X26' 'X30' 'X31' 'X32' 'X33' 'X34' 'X35' 'X36' 'X38' 'X43'\n",
      " 'X44' 'X46' 'X47' 'X48' 'X49' 'X50' 'X51' 'X53' 'X54' 'X56' 'X58' 'X62'\n",
      " 'X63' 'X64']\n"
     ]
    }
   ],
   "source": [
    "# get unique feature array\n",
    "feature_df = np.unique(corr_high_final)\n",
    "print(feature_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f0f48f",
   "metadata": {},
   "source": [
    "## Remove non correlated columns from bankruptcy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec48ac76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         X02       X06      X09       X11       X13       X14        X16  \\\n",
      "0   0.379510  0.388250  1.13890  0.249760  0.166600  0.249760   0.733780   \n",
      "1   0.499880  0.000000  1.69960  0.261140  0.158350  0.258340   0.538380   \n",
      "2   0.695920  0.000000  1.30900  0.312580  0.244350  0.309060   0.459610   \n",
      "3   0.307340  0.149880  1.05710  0.092704  0.094257  0.092704   0.398030   \n",
      "4   0.613230  0.187320  1.15590  0.187320  0.121820  0.187320   0.322110   \n",
      "5   0.497940  0.000000  1.97860  0.286450  0.148120  0.281390   0.588580   \n",
      "6   0.647440  0.000000  1.73480  0.125750  0.309630  0.111090   0.829650   \n",
      "7   0.027059  0.000000  0.65273  0.693940  1.060200  0.652400  25.575000   \n",
      "8   0.632020  0.000000  1.33320  0.043162  0.038938  0.014434   0.082138   \n",
      "9   0.838370  0.000000  2.11560  0.184540  0.075411  0.153280   0.190300   \n",
      "10  0.443550 -0.931900  4.74470  0.242270  0.195100  0.240010   2.087000   \n",
      "11  0.111480 -0.084883  0.90732 -0.024300  0.101640 -0.024300   0.483470   \n",
      "12  0.349940  0.559830  1.12680  0.332070  0.120470  0.332070   0.994440   \n",
      "13  0.198850  0.212650  1.25700  0.078063  0.310360  0.078063   0.394150   \n",
      "14  0.211310  0.010387  1.02410 -0.034653  0.004191 -0.034653   0.015671   \n",
      "15  1.154000 -0.104130  0.97767 -0.033801 -0.012483 -0.033801  -0.024924   \n",
      "16  0.299130  0.727930  1.22140  0.336190  0.206840  0.336190   1.252100   \n",
      "17  0.242310  0.021598  1.01250  0.039729  0.044190  0.039729   0.344580   \n",
      "18  0.560370  0.000000  2.27060  0.249910  0.117000  0.242910   0.474080   \n",
      "19  0.496500  0.005436  2.28270  0.266240  0.140630  0.256190   0.646540   \n",
      "\n",
      "         X17       X18      X20  ...       X51       X53      X54       X56  \\\n",
      "0    2.63490  0.249760   43.370  ...  0.378540   2.24370   2.2480  0.121960   \n",
      "1    2.00050  0.258340   87.981  ...  0.499880  17.86600  17.8660  0.121300   \n",
      "2    1.43690  0.309060   73.133  ...  0.481520   1.20980   2.0504  0.241140   \n",
      "3    3.25370  0.092704   79.788  ...  0.307340   2.45240   2.4524  0.054015   \n",
      "4    1.63070  0.187320   57.045  ...  0.565110   1.88390   2.1184  0.134850   \n",
      "5    2.00830  0.281390  107.260  ...  0.479440   3.12090   3.1927  0.139320   \n",
      "6    1.54450  0.111090   57.733  ...  0.615760   3.72970   3.7297  0.605900   \n",
      "7   36.95700  0.652400   39.978  ...  0.013323   3.46070   3.5064  0.086730   \n",
      "8    1.58220  0.014434   36.623  ...  0.425540   0.70666   0.9476  0.180110   \n",
      "9    1.19280  0.153280  109.970  ...  0.838310   8.22600   8.2260  0.079665   \n",
      "10   2.25450  0.240010   22.027  ...  0.428040   1.45050   1.4505  0.353590   \n",
      "11   8.97020 -0.024300   99.502  ...  0.111480   1.11300   1.1130 -0.102140   \n",
      "12   2.85770  0.332070   38.183  ...  0.302070   7.51920   8.0728  0.112500   \n",
      "13   5.02900  0.078063   44.446  ...  0.041664   0.91375   1.0930  0.204440   \n",
      "14   4.73240 -0.034653  105.350  ...  0.068848   1.64820   1.9460  0.023565   \n",
      "15   0.86657 -0.033801   44.032  ...  1.154000  -3.52770  -3.5277 -0.022837   \n",
      "16   3.34300  0.336190   32.984  ...  0.299130   2.87500   2.8750  0.181270   \n",
      "17   4.12700  0.039729   36.232  ...  0.214750   2.13040   2.2085  0.012367   \n",
      "18   1.78450  0.242910   14.612  ...  0.560370   1.44290   1.4429  0.107700   \n",
      "19   2.01410  0.256190   28.801  ...  0.417540   3.20750   3.3581  0.112990   \n",
      "\n",
      "        X58       X62      X63        X64  Year  Class  \n",
      "0   0.87804   82.6580   4.4158    7.42770     1      0  \n",
      "1   0.85300  107.3500   3.4000   60.98700     1      0  \n",
      "2   0.76599  134.2700   2.7185    5.20780     1      0  \n",
      "3   0.94598   86.4350   4.2228    5.54970     1      0  \n",
      "4   0.86515  127.2100   2.8692    7.89800     1      0  \n",
      "5   0.85891   88.4440   4.1269   12.29900     1      0  \n",
      "6   0.40871  129.5500   2.8173   18.35200     1      0  \n",
      "7   0.49521    7.4503  48.9910    2.32170     1      0  \n",
      "8   0.84165  116.5000   3.1330    2.56030     1      0  \n",
      "9   0.92847  144.6300   2.5236  107.67000     1      0  \n",
      "10  0.64794   32.9280  11.0850   12.36900     1      0  \n",
      "11  1.10210   76.7320   4.7568    0.68991     1      0  \n",
      "12  0.88750   38.1680   9.5629   33.41300     1      0  \n",
      "13  0.79556   60.2180   6.0613    0.28803     1      0  \n",
      "14  0.97644   31.8070  11.4750    1.65110     1      0  \n",
      "15  1.02280  182.8100   1.9966   44.29600     1      0  \n",
      "16  0.81873   60.2970   6.0534    7.79900     1      0  \n",
      "17  0.98763   41.4850   8.7984    5.35230     1      0  \n",
      "18  0.89419   90.0800   4.0519    7.45250     1      0  \n",
      "19  0.88859   66.7640   5.4670   14.54200     1      0  \n",
      "\n",
      "[20 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "#create new dataframe with high correlation features\n",
    "bankruptcy_data_final = bankruptcy_complete[feature_df].copy()\n",
    "\n",
    "# add back in the year and class features\n",
    "bankruptcy_data_final = pd.concat([bankruptcy_data_final,bankruptcy_complete['Year']], axis = 1)\n",
    "bankruptcy_data_final = pd.concat([bankruptcy_data_final,bankruptcy_complete['Class']], axis = 1)\n",
    "print(bankruptcy_data_final.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f64a91a",
   "metadata": {},
   "source": [
    "## Train test split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a248d286",
   "metadata": {},
   "source": [
    "Train test split will be done with the stardard 75/25 split and the data stratified by bankruptcy class to maintain ratio of bankrupt and non-bankrupt entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30b7204a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           X02       X06      X09       X11       X13       X14       X16  \\\n",
      "12113  1.39110 -1.630900  0.88906 -1.068200 -0.104620 -1.068200 -0.147250   \n",
      "3289   0.25636  0.035034  0.98773  0.014925  0.023540  0.014925  0.256750   \n",
      "16567  0.54284  0.188430  1.00990  0.013167  0.032714  0.013167  0.083351   \n",
      "21367  0.54860  0.297050  1.12700  0.119670  0.125330  0.119670  0.267740   \n",
      "32394  0.10553  0.390000  1.16000  0.152710  0.196260  0.152710  1.864000   \n",
      "...        ...       ...      ...       ...       ...       ...       ...   \n",
      "24299  2.15570 -0.072920  1.14410 -1.080300 -0.961970 -1.138900 -0.510570   \n",
      "41350  0.43133  0.217130  1.07930  0.143070  0.092277  0.143070  0.386680   \n",
      "38703  0.55160  0.000000  1.18680  0.137730  0.093009  0.079772  0.200110   \n",
      "12352  0.15038 -0.000078  0.99346  0.152910  0.194080  0.151780  1.282100   \n",
      "24962  0.69258 -0.054472  6.12300  0.051654  0.016724  0.032481  0.147860   \n",
      "\n",
      "           X17       X18      X20  ...      X50      X51      X53      X54  \\\n",
      "12113  0.71888 -1.068200  26.8650  ...  0.39897  1.38780 -0.97963 -0.97235   \n",
      "3289   3.90070  0.014925  25.0070  ...  2.05400  0.25636  1.54190  1.54190   \n",
      "16567  1.84220  0.013167  40.3910  ...  0.59969  0.44653  0.59774  0.74054   \n",
      "21367  1.82280  0.119670  75.4790  ...  1.10130  0.22667  1.09260  1.90590   \n",
      "32394  9.47590  0.152710  22.2240  ...  6.70350  0.10553  3.04450  3.04450   \n",
      "...        ...       ...      ...  ...      ...      ...      ...      ...   \n",
      "24299  0.46389 -1.138900   7.9212  ...  0.11009  0.93181 -1.51530  0.08941   \n",
      "41350  2.31840  0.143070  21.3420  ...  1.64960  0.43133  1.86960  1.86960   \n",
      "38703  1.81290  0.079772  84.8980  ...  0.65471  0.18832  0.70188  1.23820   \n",
      "12352  6.64980  0.151780  32.5250  ...  4.04140  0.10970  2.16600  2.16600   \n",
      "24962  1.44390  0.032481   3.4983  ...  0.42018  0.53360  0.43360  0.65036   \n",
      "\n",
      "            X56      X58      X62      X63     X64  Year  \n",
      "12113 -0.124790  1.12480  258.710   1.4109  4.4000     2  \n",
      "3289  -0.012423  1.01240   33.465  10.9070  5.9061     1  \n",
      "16567  0.009814  0.99019  117.840   3.0973  2.0506     2  \n",
      "21367  0.112700  0.88730   70.597   5.1702  2.9609     3  \n",
      "32394  0.137950  0.86205   38.432   9.4973  3.4257     4  \n",
      "...         ...      ...      ...      ...     ...   ...  \n",
      "24299 -0.919860  1.99770  297.260   1.2279  1.5002     3  \n",
      "41350  0.073493  0.92651   87.103   4.1905  6.2648     5  \n",
      "38703  0.106670  0.93618   57.917   6.3021  1.8577     5  \n",
      "12352  0.134320  0.85032   40.303   9.0564  2.5327     2  \n",
      "24962  0.007864  0.99739   31.809  11.4750  8.6363     3  \n",
      "\n",
      "[32553 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bankruptcy_data_final.drop(columns='Class'), bankruptcy_data_final.Class,\n",
    "                                                    test_size=.25  ,random_state=5, stratify=bankruptcy_data_final.Class)\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bf21a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32553, 39), (10852, 39))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display X training and test sizes\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ddb0334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32553,), (10852,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display y training and test sizes\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea6fbec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['X02', 'X06', 'X09', 'X11', 'X13', 'X14', 'X16', 'X17', 'X18', 'X20',\n",
      "       'X22', 'X23', 'X24', 'X25', 'X26', 'X30', 'X31', 'X32', 'X33', 'X34',\n",
      "       'X35', 'X36', 'X38', 'X43', 'X44', 'X46', 'X47', 'X48', 'X49', 'X50',\n",
      "       'X51', 'X53', 'X54', 'X56', 'X58', 'X62', 'X63', 'X64', 'Year'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# verify proper training columns\n",
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a676281",
   "metadata": {},
   "source": [
    "## Scale the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e96c3",
   "metadata": {},
   "source": [
    "Based on the range of the data and the degree of outliers the data was scaled before the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd8b7e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale fit and transform the data using standard scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b579f26",
   "metadata": {},
   "source": [
    "## Create the Logistic Regression object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74a75885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the regression object\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "# fit the scaled and training data\n",
    "clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae34a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the regression model\n",
    "y_te_pred = clf.predict(X_test_scaled)\n",
    "y_predict_training = clf.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb54145e",
   "metadata": {},
   "source": [
    "## Accuracy Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b79bec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Test] Accuracy score: (y_test, y_te_pred) 0.951529671949871\n",
      "\n",
      "\n",
      "[Training] Accuracy score: (y_train, y_predict_training) 0.9518631155346665\n"
     ]
    }
   ],
   "source": [
    "# print test accuracy\n",
    "print(\"\\n\")\n",
    "print(\"[Test] Accuracy score: (y_test, y_te_pred)\",accuracy_score(y_test, y_te_pred))\n",
    "\n",
    "# print training accuracy\n",
    "print(\"\\n\")\n",
    "print(\"[Training] Accuracy score: (y_train, y_predict_training)\",accuracy_score(y_train, y_predict_training))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ec2e26",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c655dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98     30985\n",
      "           1       0.52      0.01      0.02      1568\n",
      "\n",
      "    accuracy                           0.95     32553\n",
      "   macro avg       0.74      0.50      0.50     32553\n",
      "weighted avg       0.93      0.95      0.93     32553\n",
      "\n",
      "[Test Classification Report]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98     10329\n",
      "           1       0.36      0.01      0.01       523\n",
      "\n",
      "    accuracy                           0.95     10852\n",
      "   macro avg       0.66      0.50      0.50     10852\n",
      "weighted avg       0.92      0.95      0.93     10852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print classification scores  0 is non bankrupt entities 1 is bankrupt entitiies\n",
    "print(\"[Training Classification Report]\")\n",
    "print(classification_report(y_train, y_predict_training))\n",
    "\n",
    "print(\"[Test Classification Report]\")\n",
    "print(classification_report(y_test, y_te_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faae5d9",
   "metadata": {},
   "source": [
    "The overll accuracy score is .95 due to the the size of the non bankrupt entities.  The logistic regression model predicts the non bankrupt entities very well but the does not do as well with the banrkrupt entities at .52 for the training data and fares worse on the test data at .36."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e232bf1",
   "metadata": {},
   "source": [
    "## Summary and further steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff3e8b5",
   "metadata": {},
   "source": [
    "In the pre-processing step, I imputed the missing values, removed redundant correlation pairs and created the final feature dataframe.  Then I created the train test split, scaled the data and trained the logistic regression model.  Finally I checked the accuracy scores and classification report.  In the modeling step I will explore other models, evaluate performance metrics and identify the best performing model.     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
